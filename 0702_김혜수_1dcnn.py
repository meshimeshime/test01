# -*- coding: utf-8 -*-
"""0702_김혜수_1DCNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lIIquZMW7IYqyvcs3Ze8FqrgdJZTDqEm

#데이터 로드 및 전처리
"""

import os
import pandas as pd

df_merged = pd.read_csv("1m_merged.csv", parse_dates=["timestamp"])
train_df = pd.read_csv("ch2025_metrics_train.csv")
test_df = pd.read_csv('ch2025_submission_sample.csv')

train_df_01 = train_df[train_df['subject_id'] == 'id01']
train_df_02 = train_df[train_df['subject_id'] == 'id02']
train_df_03 = train_df[train_df['subject_id'] == 'id03']
train_df_04 = train_df[train_df['subject_id'] == 'id04']
train_df_05 = train_df[train_df['subject_id'] == 'id05']
train_df_06 = train_df[train_df['subject_id'] == 'id06']
train_df_07 = train_df[train_df['subject_id'] == 'id07']
train_df_08 = train_df[train_df['subject_id'] == 'id08']
train_df_09 = train_df[train_df['subject_id'] == 'id09']
train_df_10 = train_df[train_df['subject_id'] == 'id10']

test_df_01 = test_df[test_df['subject_id'] == 'id01']
test_df_02 = test_df[test_df['subject_id'] == 'id02']
test_df_03 = test_df[test_df['subject_id'] == 'id03']
test_df_04 = test_df[test_df['subject_id'] == 'id04']
test_df_05 = test_df[test_df['subject_id'] == 'id05']
test_df_06 = test_df[test_df['subject_id'] == 'id06']
test_df_07 = test_df[test_df['subject_id'] == 'id07']
test_df_08 = test_df[test_df['subject_id'] == 'id08']
test_df_09 = test_df[test_df['subject_id'] == 'id09']
test_df_10 = test_df[test_df['subject_id'] == 'id10']

df_merged['subject_id'] = df_merged['subject_id'].apply(lambda x: f'id{x:02d}')

df_merged = df_merged.fillna(0)

df_01 = df_merged[df_merged['subject_id']=='id01']
df_02 = df_merged[df_merged['subject_id']=='id02']
df_03 = df_merged[df_merged['subject_id']=='id03']
df_04 = df_merged[df_merged['subject_id']=='id04']
df_05 = df_merged[df_merged['subject_id']=='id05']
df_06 = df_merged[df_merged['subject_id']=='id06']
df_07 = df_merged[df_merged['subject_id']=='id07']
df_08 = df_merged[df_merged['subject_id']=='id08']
df_09 = df_merged[df_merged['subject_id']=='id09']
df_10 = df_merged[df_merged['subject_id']=='id10']

df_li = [df_01, df_02, df_03, df_04, df_05, df_06, df_07, df_08, df_09, df_10]

for i in df_li:
  i = i.fillna(0)

df_01.info()

"""#1D CNN 훈련"""

test_li = [test_df_01, test_df_02, test_df_03, test_df_04, test_df_05, test_df_06, test_df_07, test_df_08, test_df_09, test_df_10]
train_li = [train_df_01,train_df_02, train_df_03, train_df_04, train_df_05, train_df_06, train_df_07, train_df_08, train_df_09, train_df_10]
df_li = [df_01, df_02, df_03, df_04, df_05, df_06, df_07, df_08, df_09, df_10]

import pandas as pd
import numpy as np
from datetime import datetime
import torch
from sklearn.preprocessing import StandardScaler, MinMaxScaler

def prepare_daily_tensors_for_cnn(df, target_df, target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard'):

    # 타임스탬프를 datetime으로 변환하고 날짜 컬럼 추가
    df_copy = df.copy()
    df_copy['timestamp'] = pd.to_datetime(df_copy['timestamp'])
    df_copy['date'] = df_copy['timestamp'].dt.date

    # 타겟 데이터 준비
    target_copy = target_df.copy()
    target_copy['lifelog_date'] = pd.to_datetime(target_copy['lifelog_date']).dt.date

    # 피처 컬럼 자동 선택 (숫자형 컬럼만)
    if feature_cols is None:
        numeric_cols = df_copy.select_dtypes(include=[np.number]).columns.tolist()
        # subject_id 제외
        exclude_cols = ['subject_id']
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

    print(f"사용할 피처: {feature_cols}")

    # 일별로 그룹화
    daily_groups = df_copy.groupby(['subject_id', 'date'])

    X_data = []
    y_data = []
    daily_info = []
    matched_count = 0

    for (subject_id, date), group in daily_groups:
        # 시간순 정렬
        group = group.sort_values('timestamp')

        # 피처 데이터 추출
        features = group[feature_cols].values  # (sequence_length, n_features)

        # 해당 날짜의 타겟 데이터 찾기
        target_row = target_copy[
            (target_copy['subject_id'] == subject_id) &
            (target_copy['lifelog_date'] == date)
        ]

        if not target_row.empty:
            # 타겟 데이터가 있는 경우만 추가
            targets = target_row[target_cols].values[0]  # 첫 번째 (유일한) 행의 타겟값들

            X_data.append(features)
            y_data.append(targets)
            daily_info.append({
                'subject_id': subject_id,
                'date': date,
                'length': len(group),
                'targets': targets
            })
            matched_count += 1
        else:
            print(f"타겟 데이터 없음: {subject_id}, {date}")

    print(f"총 {len(X_data)}일의 데이터가 준비되었습니다. ({matched_count}개 매칭됨)")
    if len(X_data) == 0:
        raise ValueError("매칭되는 데이터가 없습니다. subject_id와 날짜를 확인해주세요.")

    print(f"각 일별 데이터 길이: {[len(x) for x in X_data[:5]]}...")  # 처음 5개만 출력

    # 시퀀스 길이 통일 (패딩 또는 자르기)
    max_length = max(len(x) for x in X_data)
    min_length = min(len(x) for x in X_data)
    print(f"시퀀스 길이 - 최대: {max_length}, 최소: {min_length}")

    # 고정 길이로 설정 (예: 1440분 = 24시간 * 60분, 또는 적절한 길이)
    fixed_length = min(1440, max_length)  # 최대 1440 또는 실제 최댓값 중 작은 값

    # 패딩/자르기 적용
    X_padded = []
    for x in X_data:
        if len(x) >= fixed_length:
            # 길이가 충분한 경우 마지막 부분 사용
            X_padded.append(x[-fixed_length:])
        else:
            # 길이가 부족한 경우 앞쪽을 0으로 패딩
            padding = np.zeros((fixed_length - len(x), len(feature_cols)))
            X_padded.append(np.vstack([padding, x]))

    # numpy 배열로 변환
    X_array = np.array(X_padded)  # (n_days, sequence_length, n_features)

    # 스케일링 적용
    if scaler_type is not None:
        print(f"{scaler_type} 스케일링 적용 중...")
        n_days, seq_len, n_features = X_array.shape

        # 배열을 2D로 reshape
        X_reshaped = X_array.reshape(-1, n_features)

        # 스케일러 적용
        if scaler_type == 'standard':
            scaler = StandardScaler()
        elif scaler_type == 'minmax':
            scaler = MinMaxScaler()

        X_scaled = scaler.fit_transform(X_reshaped)
        X_array = X_scaled.reshape(n_days, seq_len, n_features)

    # PyTorch 텐서로 변환 (1D CNN을 위해 transpose)
    # 1D CNN 입력 형태: (batch_size, n_features, sequence_length)
    X_tensor = torch.FloatTensor(X_array).transpose(1, 2)
    y_tensor = torch.FloatTensor(np.array(y_data))  # 타겟 데이터가 항상 존재함

    print(f"최종 텐서 형태:")
    print(f"X_tensor: {X_tensor.shape}")  # (n_days, n_features, sequence_length)
    print(f"y_tensor: {y_tensor.shape}")  # (n_days, n_targets)

    return X_tensor, y_tensor, feature_cols, daily_info

# Dataset 01
X_train_01, y_train_01, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_01, train_df_01,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_01, y_test_01, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_01, test_df_01,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

# Dataset 02
X_train_02, y_train_02, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_02, train_df_02,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_02, y_test_02, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_02, test_df_02,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

# Dataset 03
X_train_03, y_train_03, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_03, train_df_03,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_03, y_test_03, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_03, test_df_03,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

# Dataset 04
X_train_04, y_train_04, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_04, train_df_04,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_04, y_test_04, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_04, test_df_04,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

# Dataset 05
X_train_05, y_train_05, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_05, train_df_05,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_05, y_test_05, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_05, test_df_05,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

# Dataset 06
X_train_06, y_train_06, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_06, train_df_06,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_06, y_test_06, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_06, test_df_06,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

# Dataset 07
X_train_07, y_train_07, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_07, train_df_07,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_07, y_test_07, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_07, test_df_07,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

# Dataset 08
X_train_08, y_train_08, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_08, train_df_08,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_08, y_test_08, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_08, test_df_08,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

# Dataset 09
X_train_09, y_train_09, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_09, train_df_09,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_09, y_test_09, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_09, test_df_09,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

# Dataset 10
X_train_10, y_train_10, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_10, train_df_10,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')
X_test_10, y_test_10, feature_names, daily_info = prepare_daily_tensors_for_cnn(df_10, test_df_10,target_cols=['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3'],
                                 feature_cols=None, scaler_type='standard')

print("✅ All datasets converted to tensors!")
print("Available variables:")
print("Training: X_train_01~10, y_train_01~10")
print("Testing: X_test_01~10, y_test_01~10")

"""#1D CNN Test"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
import numpy as np
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, StratifiedKFold
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

class SEBlock(nn.Module):
    """Squeeze-and-Excitation Block for channel attention"""
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.expand_as(x)

class ResidualBlock(nn.Module):
    """1D Residual Block with SE attention"""
    def __init__(self, in_channels, out_channels, kernel_size=7, stride=1):
        super(ResidualBlock, self).__init__()

        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=kernel_size//2)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding=kernel_size//2)
        self.bn2 = nn.BatchNorm1d(out_channels)

        self.se = SEBlock(out_channels)

        # Shortcut connection
        if in_channels != out_channels or stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv1d(in_channels, out_channels, 1, stride),
                nn.BatchNorm1d(out_channels)
            )
        else:
            self.shortcut = nn.Identity()

        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        residual = self.shortcut(x)

        out = F.relu(self.bn1(self.conv1(x)))
        out = self.dropout(out)
        out = self.bn2(self.conv2(out))
        out = self.se(out)

        out += residual
        out = F.relu(out)
        return out

class MultiScaleFeatureExtractor(nn.Module):
    """Multi-scale feature extraction using different kernel sizes"""
    def __init__(self, in_channels, out_channels):
        super(MultiScaleFeatureExtractor, self).__init__()

        # Different kernel sizes for multi-scale features
        self.conv_3 = nn.Conv1d(in_channels, out_channels//4, 3, padding=1)
        self.conv_7 = nn.Conv1d(in_channels, out_channels//4, 7, padding=3)
        self.conv_15 = nn.Conv1d(in_channels, out_channels//4, 15, padding=7)
        self.conv_31 = nn.Conv1d(in_channels, out_channels//4, 31, padding=15)

        self.bn = nn.BatchNorm1d(out_channels)

    def forward(self, x):
        f1 = self.conv_3(x)
        f2 = self.conv_7(x)
        f3 = self.conv_15(x)
        f4 = self.conv_31(x)

        out = torch.cat([f1, f2, f3, f4], dim=1)
        out = F.relu(self.bn(out))
        return out

class AdvancedCNN(nn.Module):
    """Advanced CNN with multiple techniques for robustness"""
    def __init__(self, input_channels=20, sequence_length=1440, num_outputs=6):
        super(AdvancedCNN, self).__init__()

        # Initial multi-scale feature extraction
        self.multi_scale = MultiScaleFeatureExtractor(input_channels, 64)

        # Residual blocks with increasing channels
        self.res_block1 = ResidualBlock(64, 64, kernel_size=15, stride=2)
        self.res_block2 = ResidualBlock(64, 96, kernel_size=11, stride=2)
        self.res_block3 = ResidualBlock(96, 128, kernel_size=7, stride=2)

        # Global average pooling with different scales
        self.gap1 = nn.AdaptiveAvgPool1d(1)
        self.gap2 = nn.AdaptiveAvgPool1d(4)
        self.gap3 = nn.AdaptiveAvgPool1d(16)

        # Feature fusion
        self.feature_fusion = nn.Sequential(
            nn.Linear(128 * (1 + 4 + 16), 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # Multi-task learning with shared representation
        self.shared_layer = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # Task-specific heads with different architectures
        self.q_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Dropout(0.15),
                nn.Linear(32, 16),
                nn.ReLU(),
                nn.Linear(16, 3)
            ) for _ in range(3)
        ])

        self.s_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Dropout(0.15),
                nn.Linear(32, 16),
                nn.ReLU(),
                nn.Linear(16, 3)
            ) for _ in range(3)
        ])

    def forward(self, x):
        # Multi-scale feature extraction
        x = self.multi_scale(x)

        # Residual blocks
        x = self.res_block1(x)
        x = self.res_block2(x)
        x = self.res_block3(x)

        # Multi-scale global pooling
        gap1 = self.gap1(x).flatten(1)
        gap2 = self.gap2(x).flatten(1)
        gap3 = self.gap3(x).flatten(1)

        # Feature fusion
        features = torch.cat([gap1, gap2, gap3], dim=1)
        features = self.feature_fusion(features)

        # Shared representation
        shared = self.shared_layer(features)

        # Task-specific outputs
        q_outputs = [head(shared) for head in self.q_heads]
        s_outputs = [head(shared) for head in self.s_heads]

        all_outputs = q_outputs + s_outputs
        return torch.stack(all_outputs, dim=1)

class EnhancedRobustTrainer:
    """Enhanced Robust training with advanced augmentation techniques"""
    def __init__(self):
        self.models = {}
        self.best_scores = {}

    def mixup_data(self, x, y, alpha=0.2):
        """Mixup data augmentation"""
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1

        batch_size = x.size(0)
        index = torch.randperm(batch_size)

        mixed_x = lam * x + (1 - lam) * x[index, :]
        y_a, y_b = y, y[index]
        return mixed_x, y_a, y_b, lam

    def cutmix_data(self, x, y, alpha=1.0):
        """CutMix augmentation for time series"""
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1

        batch_size = x.size(0)
        index = torch.randperm(batch_size)

        # Cut and mix along time dimension
        seq_len = x.size(2)
        cut_len = int(seq_len * (1 - lam))
        cut_start = np.random.randint(0, seq_len - cut_len + 1)

        mixed_x = x.clone()
        mixed_x[:, :, cut_start:cut_start+cut_len] = x[index, :, cut_start:cut_start+cut_len]

        y_a, y_b = y, y[index]
        return mixed_x, y_a, y_b, lam

    def mixup_criterion(self, pred, y_a, y_b, lam, criterion):
        """Mixup loss calculation"""
        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

    def advanced_augmentation(self, X, y, augment_factor=6):
        """강화된 데이터 증강 - 피처가 줄어든 만큼 더 다양한 증강"""
        augmented_X = [X]
        augmented_y = [y]

        for i in range(augment_factor - 1):
            # 1. Gaussian noise with varying intensities
            noise_std = np.random.uniform(0.02, 0.08)
            noise = torch.randn_like(X) * noise_std
            augmented_X.append(X + noise)
            augmented_y.append(y)

            # 2. Time shifting with wrap-around
            shift = np.random.randint(-200, 200)
            shifted_X = torch.roll(X, shifts=shift, dims=2)
            augmented_X.append(shifted_X)
            augmented_y.append(y)

            # 3. Scaling with channel-specific factors
            scale_factors = torch.rand(1, X.shape[1], 1) * 0.6 + 0.7  # 0.7-1.3
            scaled_X = X * scale_factors
            augmented_X.append(scaled_X)
            augmented_y.append(y)

            # 4. Time masking (random intervals)
            masked_X = X.clone()
            num_masks = np.random.randint(1, 4)
            for _ in range(num_masks):
                mask_length = np.random.randint(30, 150)
                mask_start = np.random.randint(0, X.shape[2] - mask_length)
                masked_X[:, :, mask_start:mask_start+mask_length] = 0
            augmented_X.append(masked_X)
            augmented_y.append(y)

            # 5. Channel dropout
            channel_drop_X = X.clone()
            num_channels_to_drop = np.random.randint(1, 3)
            channels_to_drop = np.random.choice(X.shape[1], num_channels_to_drop, replace=False)
            channel_drop_X[:, channels_to_drop, :] = 0
            augmented_X.append(channel_drop_X)
            augmented_y.append(y)

            # 6. Frequency domain augmentation (FFT based)
            fft_X = torch.fft.fft(X, dim=2)
            # Add noise in frequency domain
            freq_noise = torch.randn_like(fft_X) * 0.1
            fft_X_noisy = fft_X + freq_noise
            freq_aug_X = torch.fft.ifft(fft_X_noisy, dim=2).real
            augmented_X.append(freq_aug_X)
            augmented_y.append(y)

            # 7. Smooth random warping
            time_steps = torch.linspace(0, 1, X.shape[2])
            warp_strength = np.random.uniform(0.05, 0.15)
            warp = torch.sin(2 * np.pi * np.random.uniform(1, 3) * time_steps) * warp_strength
            warp_indices = torch.clamp((time_steps + warp) * (X.shape[2] - 1), 0, X.shape[2] - 1).long()
            warped_X = X[:, :, warp_indices]
            augmented_X.append(warped_X)
            augmented_y.append(y)

            # 8. Amplitude modulation
            modulation_freq = np.random.uniform(0.1, 2.0)
            modulation = 1 + 0.2 * torch.sin(2 * np.pi * modulation_freq * time_steps)
            modulated_X = X * modulation.unsqueeze(0).unsqueeze(0)
            augmented_X.append(modulated_X)
            augmented_y.append(y)

        return torch.cat(augmented_X, dim=0), torch.cat(augmented_y, dim=0)

    def focal_loss(self, pred, target, alpha=0.25, gamma=2.0):
        """Focal loss for class imbalance"""
        ce_loss = F.cross_entropy(pred, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = alpha * (1-pt)**gamma * ce_loss
        return focal_loss.mean()

    def label_smoothing_loss(self, pred, target, smoothing=0.1):
        """Label smoothing for regularization"""
        confidence = 1.0 - smoothing
        true_dist = torch.zeros_like(pred)
        true_dist.fill_(smoothing / (pred.size(-1) - 1))
        true_dist.scatter_(1, target.unsqueeze(1), confidence)
        return F.kl_div(F.log_softmax(pred, dim=1), true_dist, reduction='batchmean')

    def train_with_kfold(self, person_id, X_train, y_train, X_test=None, n_splits=3):
        """K-fold cross validation training with enhanced augmentation"""
        print(f"\n{'='*60}")
        print(f"Training Enhanced CNN for Person {person_id} with {n_splits}-Fold CV")
        print(f"{'='*60}")

        # Data info
        print(f"Original data shape: {X_train.shape}")

        # Class distribution
        print("\nClass distribution:")
        for i in range(6):
            counts = torch.bincount(y_train[:, i].long(), minlength=3)
            output_name = f"Q{i+1}" if i < 3 else f"S{i-2}"
            print(f"  {output_name}: {counts.tolist()}")

        # K-fold cross validation
        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
        fold_scores = []
        fold_models = []

        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train[:, 0])):
            print(f"\nFold {fold + 1}/{n_splits}")

            # Split data
            X_tr_orig = X_train[train_idx]
            y_tr_orig = y_train[train_idx]
            X_val = X_train[val_idx]
            y_val = y_train[val_idx]

            # Enhanced augmentation - 더 강한 증강 적용
            augment_factor = 8 if len(X_tr_orig) < 50 else (6 if len(X_tr_orig) < 100 else 4)
            print(f"  Applying enhanced augmentation (factor: {augment_factor}) to training set (size: {len(X_tr_orig)})")
            X_tr, y_tr = self.advanced_augmentation(X_tr_orig, y_tr_orig, augment_factor=augment_factor)
            print(f"  Augmented training set size: {len(X_tr)}")

            print(f"  Training set: {X_tr.shape}, Validation set: {X_val.shape}")

            # Model and optimizer
            model = AdvancedCNN()
            optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.01)
            scheduler = torch.optim.lr_scheduler.OneCycleLR(
                optimizer, max_lr=0.002, steps_per_epoch=len(X_tr)//16+1, epochs=200
            )

            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model = model.to(device)

            # Training with enhanced techniques
            best_val_f1 = 0
            best_model_state = None
            patience_counter = 0

            for epoch in range(200):
                model.train()

                # Mini-batch training with multiple augmentation techniques
                indices = torch.randperm(len(X_tr))
                for i in range(0, len(X_tr), 16):
                    batch_idx = indices[i:i+16]
                    if len(batch_idx) == 0:
                        continue

                    batch_X = X_tr[batch_idx].to(device)
                    batch_y = y_tr[batch_idx].long().to(device)

                    # Random augmentation selection
                    aug_type = np.random.choice(['mixup', 'cutmix', 'none'], p=[0.4, 0.3, 0.3])

                    if aug_type == 'mixup':
                        mixed_X, y_a, y_b, lam = self.mixup_data(batch_X, batch_y, alpha=0.3)
                        outputs = model(mixed_X)
                        loss = 0
                        for j in range(6):
                            loss_a = self.focal_loss(outputs[:, j, :], y_a[:, j])
                            loss_b = self.focal_loss(outputs[:, j, :], y_b[:, j])
                            loss += lam * loss_a + (1 - lam) * loss_b
                    elif aug_type == 'cutmix':
                        mixed_X, y_a, y_b, lam = self.cutmix_data(batch_X, batch_y, alpha=1.0)
                        outputs = model(mixed_X)
                        loss = 0
                        for j in range(6):
                            loss_a = self.label_smoothing_loss(outputs[:, j, :], y_a[:, j], smoothing=0.1)
                            loss_b = self.label_smoothing_loss(outputs[:, j, :], y_b[:, j], smoothing=0.1)
                            loss += lam * loss_a + (1 - lam) * loss_b
                    else:
                        outputs = model(batch_X)
                        loss = 0
                        for j in range(6):
                            if np.random.random() > 0.5:
                                loss += self.focal_loss(outputs[:, j, :], batch_y[:, j])
                            else:
                                loss += self.label_smoothing_loss(outputs[:, j, :], batch_y[:, j], smoothing=0.1)

                    loss = loss / 6

                    optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                    optimizer.step()
                    scheduler.step()

                # Validation
                if epoch % 15 == 0:
                    model.eval()
                    with torch.no_grad():
                        X_val_dev = X_val.to(device)
                        val_outputs = model(X_val_dev)
                        val_preds = torch.argmax(val_outputs, dim=2)

                        f1_scores = []
                        for j in range(6):
                            f1 = f1_score(
                                y_val[:, j].cpu().numpy(),
                                val_preds[:, j].cpu().numpy(),
                                average='weighted',
                                zero_division=0
                            )
                            f1_scores.append(f1)

                        avg_f1 = np.mean(f1_scores)

                        if avg_f1 > best_val_f1:
                            best_val_f1 = avg_f1
                            best_model_state = model.state_dict().copy()
                            patience_counter = 0
                        else:
                            patience_counter += 1

                        if patience_counter >= 25:  # Increased patience
                            break

            # Store fold results
            model.load_state_dict(best_model_state)
            fold_scores.append(best_val_f1)
            fold_models.append(model.state_dict().copy())

            print(f"  Fold {fold + 1} F1: {best_val_f1:.4f}")

        # Select best fold model
        best_fold_idx = np.argmax(fold_scores)
        best_model = AdvancedCNN()
        best_model.load_state_dict(fold_models[best_fold_idx])

        avg_score = np.mean(fold_scores)
        print(f"\nK-Fold Results:")
        print(f"  Average F1: {avg_score:.4f} (±{np.std(fold_scores):.4f})")
        print(f"  Best Fold F1: {max(fold_scores):.4f}")

        # Store results
        self.models[person_id] = best_model
        self.best_scores[person_id] = avg_score

        # Test prediction
        if X_test is not None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            best_model = best_model.to(device)
            best_model.eval()

            with torch.no_grad():
                X_test_dev = X_test.to(device)
                test_outputs = best_model(X_test_dev)
                test_preds = torch.argmax(test_outputs, dim=2)
                return test_preds.cpu()

        return None

    def train_all_persons(self, train_data_list, test_data_list):
        """Train all person models with enhanced robust techniques"""
        all_predictions = []

        print("="*70)
        print("ENHANCED ROBUST CNN TRAINING (5 Features)")
        print("="*70)

        for i, ((X_train, y_train), X_test) in enumerate(
            zip(train_data_list, test_data_list), 1
        ):
            predictions = self.train_with_kfold(i, X_train, y_train, X_test)
            if predictions is not None:
                all_predictions.append(predictions)

        # Summary
        print("\n" + "="*70)
        print("ENHANCED TRAINING SUMMARY")
        print("="*70)

        scores = list(self.best_scores.values())
        print(f"Average F1 across all persons: {np.mean(scores):.4f}")
        print(f"Best performing person: Person {np.argmax(scores)+1} (F1={max(scores):.4f})")
        print(f"Worst performing person: Person {np.argmin(scores)+1} (F1={min(scores):.4f})")
        print(f"Standard deviation: {np.std(scores):.4f}")

        return all_predictions

# Main execution
def main():
    # Prepare data
    train_data = [
        (X_train_01, y_train_01), (X_train_02, y_train_02),
        (X_train_03, y_train_03), (X_train_04, y_train_04),
        (X_train_05, y_train_05), (X_train_06, y_train_06),
        (X_train_07, y_train_07), (X_train_08, y_train_08),
        (X_train_09, y_train_09), (X_train_10, y_train_10)
    ]

    test_data = [
        X_test_01, X_test_02, X_test_03, X_test_04, X_test_05,
        X_test_06, X_test_07, X_test_08, X_test_09, X_test_10
    ]

    # Enhanced Training
    trainer = EnhancedRobustTrainer()
    predictions = trainer.train_all_persons(train_data, test_data)

    # Combine results
    final_predictions = torch.cat(predictions, dim=0)
    print(f"\nFinal predictions shape: {final_predictions.shape}")

    # Save to CSV
    import pandas as pd
    pred_array = final_predictions.numpy()

    df = pd.DataFrame()
    df['Q1'] = pred_array[:, 0]
    df['Q2'] = pred_array[:, 1]
    df['Q3'] = pred_array[:, 2]
    df['S1'] = pred_array[:, 3]
    df['S2'] = pred_array[:, 4]
    df['S3'] = pred_array[:, 5]

    df.to_csv('enhanced_5feature_cnn_predictions.csv', index=False)
    print("\nPredictions saved to '0704_cnn_predictions.csv'")

    return final_predictions, trainer

if __name__ == "__main__":
    predictions, trainer = main()

"""#사용자 1,2,5,8,9 개선"""

